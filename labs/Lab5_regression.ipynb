{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u0yZqFCyVwy"
      },
      "source": [
        "#Laborator 5\n",
        "\n",
        "In cadrul acestui laborator vom implementa o solutie de detectie. Datasetul folosit se numeste [American Sign Language Letters Dataset](https://public.roboflow.com/object-detection/american-sign-language-letters/1/download/coco)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czyIhYt5gmUQ"
      },
      "source": [
        "!curl -L \"https://public.roboflow.com/ds/KamceLFGGS?key=dqp8HADMki\" > roboflow.zip; unzip -q roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA7YWXQ2Jp1I"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BVgY9gr2sbr"
      },
      "source": [
        "!ls train | grep \"json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOA4ted_hHdB"
      },
      "source": [
        "## Crearea Dataloader-ului\n",
        "\n",
        "In continuare, pentru a incarca date, folosim un obiect de tipul torch.utils.data.Dataset. Acesta are 3 metode importante:\n",
        "\n",
        "```\n",
        "__init__()\n",
        "__len__()\n",
        "__get_item__()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vhpi3uC55N6"
      },
      "source": [
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "import torch as t\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import to_tensor, normalize\n",
        "import random\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "class HandsDataset(Dataset):\n",
        "  def __init__(self, coco_root, coco_annos, coco_imgs, img_size=(320, 320)):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dataset_lines (array): array of strings of form \"{image_path}|{rectangle_coordinates}\".\n",
        "        width (int): target image width.\n",
        "        height (int): target image height.\n",
        "    \"\"\"\n",
        "    self.coco_root = coco_root\n",
        "    self.coco_annos = coco_annos\n",
        "    self.coco_imgs = coco_imgs\n",
        "\n",
        "    self.coco_anno_file = os.path.join(coco_root, coco_annos)\n",
        "    self.coco_imgs_dir = os.path.join(coco_root, coco_imgs)\n",
        "\n",
        "    self.coco = COCO(self.coco_anno_file)\n",
        "\n",
        "    self.img_size = img_size\n",
        "\n",
        "    self.init_dataset()\n",
        "\n",
        "  def init_dataset(self):\n",
        "    self.cat_ids = self.coco.getCatIds()\n",
        "\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "    self.ann_ids = self.coco.getAnnIds(self.img_ids)\n",
        "\n",
        "    print(\"Dataset size {}\".format(len(self.img_ids)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_meta = self.coco.loadImgs(self.img_ids[idx])[0]\n",
        "    img_path = os.path.join(self.coco_imgs_dir, img_meta['file_name'])\n",
        "    img = Image.open(img_path)\n",
        "    ann_id = self.coco.getAnnIds(self.img_ids[idx])\n",
        "    annos = self.coco.loadAnns(ann_id)[0]\n",
        "\n",
        "    original_width, original_height = img.size\n",
        "\n",
        "    img = img.resize(self.img_size)\n",
        "    img = np.array(img)\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.expand_dims(img, axis=2)\n",
        "      img = np.repeat(img, 3, axis=2)\n",
        "\n",
        "    img = to_tensor(img)\n",
        "\n",
        "    bbox = annos['bbox']  # box is xywh\n",
        "    cat_id = annos['category_id']\n",
        "\n",
        "    x1, y1, w, h = bbox\n",
        "    x2, y2 = x1 + w, y1 + h\n",
        "\n",
        "    # x1 = x1 / original_width * self.img_size[0]\n",
        "    # x2 = x2 / original_width * self.img_size[0]\n",
        "    # y1 = y1 / original_height * self.img_size[1]\n",
        "    # y2 = y2 / original_height * self.img_size[1]\n",
        "\n",
        "    x1 = x1 / original_width\n",
        "    x2 = x2 / original_width\n",
        "    y1 = y1 / original_height\n",
        "    y2 = y2 / original_height\n",
        "\n",
        "    coordinates = np.array([x1, y1, x2, y2])\n",
        "    coordinates =  coordinates.astype(np.float32)\n",
        "\n",
        "    return img, coordinates, cat_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkijQD0uK85D"
      },
      "source": [
        "Construire Dataset si vizualizare date.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGHfd229hRyR"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "coco_root = \"\"\n",
        "coco_anno_train = os.path.join(coco_root, \"train/_annotations.coco.json\")\n",
        "coco_imgs_train = os.path.join(coco_root, \"train\")\n",
        "\n",
        "coco_anno_valid = os.path.join(coco_root, \"valid/_annotations.coco.json\")\n",
        "coco_imgs_valid = os.path.join(coco_root, \"valid\")\n",
        "\n",
        "dataset_train = HandsDataset(coco_root, coco_anno_train, coco_imgs_train, img_size=(224, 224))\n",
        "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=1)\n",
        "\n",
        "dataset_valid = HandsDataset(coco_root, coco_anno_valid, coco_imgs_valid, img_size=(224, 224))\n",
        "valid_loader = DataLoader(dataset_valid, batch_size=1, shuffle=False, num_workers=1) # keep batch_size=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "see_examples = 10\n",
        "for i, (imgs, coordinates, cat_id) in enumerate(train_loader):\n",
        "    clear_output(wait=True)\n",
        "    imgs = np.transpose(imgs, (0, 2, 3, 1))\n",
        "    print(imgs.shape)\n",
        "\n",
        "    plt.imshow(imgs[0])\n",
        "\n",
        "    x1, y1, x2, y2 = coordinates[0]\n",
        "    x1 = x1 * dataset_train.img_size[1]\n",
        "    y1 = y1 * dataset_train.img_size[0]\n",
        "    x2 = x2 * dataset_train.img_size[1]\n",
        "    y2 = y2 * dataset_train.img_size[0]\n",
        "\n",
        "    rect = patches.Rectangle((x1,y1),x2-x1+1,y2-y1+1,linewidth=1,edgecolor='r',facecolor='none')\n",
        "    plt.gca().add_patch(rect)\n",
        "    plt.show()\n",
        "\n",
        "    if i >= see_examples - 1:\n",
        "      break\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "UibCKTqfBfv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnV6PIC1kQMi"
      },
      "source": [
        "# Crearea unei retele neurale convolutionale\n",
        "\n",
        "### Cerinte\n",
        "* Creati o arhitectura de retea neuronala convolutionala pentru regresie pe cele 4 coordonate alte imaginilor din dataset.\n",
        "* Punctaj: 7 puncte pentru o retea cu rezultate *bune*.\n",
        "\n",
        "#### Hint\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "*   Numarul de exemple de antrenare este prea mic pentru a antrena o retea \"from scratch\". Folositi o retea prea-antrenata pe ImagetNet, care a invatat deja sa recunoasca trasaturi utile pentru detectia de obiecte. Arhitectura recomandata este ResNet18 din Pytorch. Alte arhitecturi pot fi incercate.\n",
        "*   La final trebuie utilizata o functie de activare care acopera [0,1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK0Z9NeYTghv"
      },
      "source": [
        "### Definirea obiectelor folosite in timpul antrenarii\n",
        "  * Numarul de epoci\n",
        "  * Retea\n",
        "  * Optimizator\n",
        "  * Functie de loss\n",
        "\n",
        "Experimentati cu valorile hiper-parametrilor de mai sus astfel incat reteaua sa invete *bine*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVixJ_VAO4qH"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Definiti numarul de epoci\n",
        "epochs = None\n",
        "\n",
        "# Definiti reteaua\n",
        "network = None\n",
        "\n",
        "# Definiti optimizatorul\n",
        "optimizer = None\n",
        "# Dupa definirea optimizatorului si dupa fiecare iteratie trebuie apelata functia zero_grad().\n",
        "# Aceasta face toti gradientii zero.\n",
        "# Completati codul pentru a face gradientii zero aici\n",
        "\n",
        "\n",
        "# Definiti functia cost pentru pentru regressie\n",
        "loss_fn = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAnUsWYWodb4"
      },
      "source": [
        "## Definirea functiei de antrenare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9MTYanoMZ8H"
      },
      "source": [
        "def bb_intersection_over_union(boxA, boxB):\n",
        "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
        "  # import pdb; pdb.set_trace()\n",
        "  xA = max(boxA[0], boxB[0])\n",
        "  yA = max(boxA[1], boxB[1])\n",
        "  xB = min(boxA[2], boxB[2])\n",
        "  yB = min(boxA[3], boxB[3])\n",
        "\t# compute the area of intersection rectangle\n",
        "  interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "  # compute the area of both the prediction and ground-truth\n",
        "  # rectangles\n",
        "  boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "  boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "  # compute the intersection over union by taking the intersection\n",
        "  # area and dividing it by the sum of prediction + ground-truth\n",
        "  # areas - the interesection area\n",
        "  iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "  # return the intersection over union value\n",
        "  return iou\n",
        "\n",
        "def train_fn(epochs: int, train_loader: DataLoader, test_loader: DataLoader, \n",
        "             net: torch.nn.Module, loss_fn: torch.nn.Module, optimizer: optim.Optimizer):\n",
        "  # Iteram prin numarul de epoci\n",
        "  for e in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    # Iteram prin fiecare exemplu din dataset\n",
        "    net.train()\n",
        "    for idx, (images, labels, _) in enumerate(train_loader):\n",
        "\n",
        "      images = images.cuda()\n",
        "      # Aplicam reteaua neurala pe imaginile de intrare\n",
        "      out = net(images)\n",
        "      # Aplicam functia cost pe iesirea retelei neurale si pe adnotarile imaginilor \n",
        "      loss = loss_fn(out, labels.cuda())\n",
        "      # Aplicam algoritmul de back-propagation\n",
        "      loss.backward()\n",
        "      # Facem pasul de optimizare, pentru a aplica gradientii pe parametrii retelei\n",
        "      optimizer.step()\n",
        "      # Apelam functia zero_grad() pentru a uita gradientii de la iteratie curenta\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      train_loss += loss.item()*images.size(0)\n",
        "\n",
        "    \n",
        "    # Caluculul acuratetii\n",
        "    count = len(test_loader)\n",
        "    IOU_list = []\n",
        "    net.eval()\n",
        "\n",
        "    for test_image, box_gt, _ in test_loader:\n",
        "      test_image = test_image.cuda()\n",
        "      bbox_pred = net(test_image)\n",
        "\n",
        "      loss = loss_fn(bbox_pred, box_gt.cuda())\n",
        "\n",
        "      bbox_pred = bbox_pred.detach().cpu().numpy()\n",
        "\n",
        "      IOU_list.append(bb_intersection_over_union(bbox_pred[0], box_gt[0]))\n",
        "\n",
        "      valid_loss += loss.item()*test_image.size(0)\n",
        "\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "    print(f\"Average train loss : {train_loss}\")\n",
        "    print(f\"Average valid loss : {valid_loss}\")\n",
        "\n",
        "    IOU_list = np.array(IOU_list)\n",
        "    IOU_list_50 = (IOU_list > 0.5).sum()\n",
        "    IOU_list_75 = (IOU_list > 0.75).sum()\n",
        "    IOU_list_90 = (IOU_list > 0.90).sum()\n",
        "\n",
        "    print(\"Acuratetea IOU 50% la finalul epocii {} este {:.2f}%\".format(e, (IOU_list_50 / count)))\n",
        "    print(\"Acuratetea IOU 75% la finalul epocii {} este {:.2f}%\".format(e, (IOU_list_75 / count)))\n",
        "    print(\"Acuratetea IOU 90% la finalul epocii {} este {:.2f}%\".format(e, (IOU_list_90 / count)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqUwOWmDMpqQ"
      },
      "source": [
        "train_fn(epochs, train_loader, valid_loader, network, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMOQhLY5Folz"
      },
      "source": [
        "## Augmentare date\n",
        "\n",
        "O metoda de imbunatatire a performantei modelului este augmentarea setului de date. Aceste augmentari trebuie sa tina cont de natura taskului si de natura etichetelor.\n",
        "\n",
        "### Cerinta\n",
        "\n",
        "(3p) Modificati Dataset-ul precedent astfel incat sa augmenteze datele de antrenare cu o probabilitate aleasa de voi. Cum afecteaza acest lucru performanta modelului?\n",
        "\n",
        "### Bonus\n",
        "(2p) Dataloaderul intoarce si clasa fiecarui obiect de detectat (litera corespunzatoare fiecarui semn din limbajul Sign Language). Creati o arhitectura care prezice si acesta clasa, pe langa regresia pe box. Antrenati reteaua. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLiuxyDZhVNG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}