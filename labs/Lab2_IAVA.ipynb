{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f32851025d2e42b89facf894eb731c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a1b7575682149bbb1500dbf279a3120",
              "IPY_MODEL_639fd14e413843feb28e1ddffc945101",
              "IPY_MODEL_8e8dd6762f7d444a973609bfdd2aa7f8"
            ],
            "layout": "IPY_MODEL_10981ae0919f4b398ac3982edf59f1d1"
          }
        },
        "6a1b7575682149bbb1500dbf279a3120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5c41780b8774ca5832aeb414d05a9ae",
            "placeholder": "​",
            "style": "IPY_MODEL_688c4da68ad74a87b957e1ee8676c673",
            "value": "100%"
          }
        },
        "639fd14e413843feb28e1ddffc945101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd700a5851f849ae8810a04b32fda061",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43142e789d64446dbadeed06cc54fb94",
            "value": 170498071
          }
        },
        "8e8dd6762f7d444a973609bfdd2aa7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87d9291e4b214379aeed908dec5b1c07",
            "placeholder": "​",
            "style": "IPY_MODEL_11f1bf86e5114dcda6bd6081b53fdc36",
            "value": " 170498071/170498071 [00:01&lt;00:00, 97188482.16it/s]"
          }
        },
        "10981ae0919f4b398ac3982edf59f1d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c41780b8774ca5832aeb414d05a9ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688c4da68ad74a87b957e1ee8676c673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd700a5851f849ae8810a04b32fda061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43142e789d64446dbadeed06cc54fb94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87d9291e4b214379aeed908dec5b1c07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f1bf86e5114dcda6bd6081b53fdc36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPquMWOmYa_D"
      },
      "source": [
        "# Antrenarea unei retele de clasificare\n",
        "\n",
        "Obiectivul acestui laborator este de a introduce conceptele de baza necesare antrenarii unei retele neuronale. Pytorch ofera posibilitatea de a incarca si procesa setul de date rapid si eficient. In acest laborator vom folosi setul de date CIFAR-10, pentru care vom rezolva problema de clasificare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzgFBIRQ1YOc"
      },
      "source": [
        "Importarea bibliotecilor care vor fi folosite in acest laborator:\n",
        "\n",
        " * *matplotlib.pyplot* pentru grafice\n",
        " * *torch.optim* pentru optimizatori\n",
        " * *torch.nn* pentru lucrul cu retele neurale\n",
        " * *torch.utils.data* pentru lucrul cu seturi de date\n",
        " * *torchvision* pentru seturi de date oferite de repository-ul Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8uJfA201Bif"
      },
      "source": [
        "from IPython import display as dspl\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7hHS0O7nI4y"
      },
      "source": [
        "## Dataset (1p)\n",
        "Clasa *torchvision.datasets.CIFAR10* este o subclasa a clasei abstracte *torch.utils.data.Dataset*. O astfel de clasa este folosita pentru a ingloba datasetul si pentru a returna elemente din dataset.\n",
        "\n",
        "O clasa derivata din *torch.utils.data.Dataset*, trebuie sa suprascrie 2 metode:\n",
        " * \\_\\_len\\_\\_(self) -> aceasta metoda returneaza numarul de elemente din dataset si permite folosirea functiei __len()__ din Python.\n",
        " * \\_\\_getitem\\_\\_ -> permite folosirea operatorului de indexare din Python __[ ]__ pentru a obtine un element de la un anumit index din dataset \n",
        "\n",
        "\n",
        "Exemplu clasa derivata din torch.utils.data.Dataset: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "\n",
        "### Cerinte\n",
        "  1. Printati numarul de exemple din datasetul de antrenare si de test (0.25p)\n",
        "  2. Printati exemplul de la indexul 0 (0.25p)\n",
        "  3. Printati valoarea maxima si valoarea minima din prima imagine din datasetul de test (0.5p)\n",
        "\n",
        "#### Hints\n",
        " * Un exemplu din dataset este reprezentat de un tuplu care contine o imagine de tip de date PIL.Image si un int reprezentand clasa imaginii\n",
        " * np.min(a) -> returneaza minimul dintr-un obiect de tipul np.ndarray\n",
        " * np.max(a) -> returneaza maximul dintr-un obiect de tipul np.ndarray\n",
        " * np.asarray(a) -> returneaza un obiect de tipul np.ndarray. Functia trebuie sa primeaca un obiect 'array-like'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvoP1C_3-3dA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "f32851025d2e42b89facf894eb731c10",
            "6a1b7575682149bbb1500dbf279a3120",
            "639fd14e413843feb28e1ddffc945101",
            "8e8dd6762f7d444a973609bfdd2aa7f8",
            "10981ae0919f4b398ac3982edf59f1d1",
            "a5c41780b8774ca5832aeb414d05a9ae",
            "688c4da68ad74a87b957e1ee8676c673",
            "cd700a5851f849ae8810a04b32fda061",
            "43142e789d64446dbadeed06cc54fb94",
            "87d9291e4b214379aeed908dec5b1c07",
            "11f1bf86e5114dcda6bd6081b53fdc36"
          ]
        },
        "outputId": "9f320542-6713-43d8-903d-35231494d3a8"
      },
      "source": [
        "# Crearea instantelor pentru setul de date CIFAR de train si de test\n",
        "cifar_train = torchvision.datasets.CIFAR10(\"./data\", download=True)\n",
        "cifar_test = torchvision.datasets.CIFAR10(\"./data\", train=False)\n",
        "\n",
        "#TODO: Scrieti aici codul pentru cerinta numarul 1\n",
        "\n",
        "#TODO: Scrieti aici codul pentru cerinta numarul 2\n",
        "\n",
        "#TODO: Completati sub codul pentru cerinta numarul 3\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f32851025d2e42b89facf894eb731c10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO0Ju49dp_xp"
      },
      "source": [
        "## Iterare prin Dataset (1p)\n",
        "\n",
        "Deoarece clasa datasetului implementeaza functia \\_\\_getitem\\_\\_(), se poate itera prin dataset cum se poate itera si printr-o lista sau alt obiect iterabil. \n",
        "\n",
        "### Cerinte\n",
        " * Odata la n pasi, printati clasa exemplului curent si afisati imaginea.\n",
        "\n",
        "#### Hints\n",
        "  * A fost importata libraria matplotlib.pyplot as plt\n",
        "  * Functia plt.figure(figsize=(float, float)) returneaza o figura de dimensiunea oferita ca parametru in *figsize*\n",
        "  * Functia plt.imshow(np.ndarray) plaseaza o imagine pe o figura\n",
        "  * Functia plt.show() afiseaza figura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v_DcyDBHVQu"
      },
      "source": [
        "n = 10000\n",
        "\n",
        "for idx, example in enumerate(cifar_train):\n",
        "  if idx % n == 0:\n",
        "    # Aceasta functie sterge ce a fost afisat pana la momentul curent\n",
        "    dspl.clear_output(wait=True)\n",
        "  \n",
        "    #TODO: Completati codul aici si afisati cateva imagini din dataset\n",
        "    \n",
        "    # Aceasta functie opreste procesul pentru 2 secunde \n",
        "    time.sleep(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iud_GoZJwK9U"
      },
      "source": [
        "## DataLoader (1p)\n",
        "\n",
        "In torch.utils.data este definita clasa *DataLoader*. Aceasta este un wrapper peste o clasa de tip Dataset si este folosita pentru a abstractiza procesarea pe mai mult threaduri, concatenarea exemplelor in batch-uri, si extragerea exemplelor in mod aleatoriu din dataset.\n",
        "\n",
        "Pentru a crea un obiect de tip *DataLoader* se foloseste constructorul care are urmatorul header:\n",
        "\n",
        "  \\_\\_init\\_\\_(dataset_object, batch_size=1, shuffle=False, num_workers=0, collate_fn=None)\n",
        "\n",
        " * dataset_object - obiectul de tip Dataset care va fi inglobat\n",
        " * batch_size - dimensiunea batch-ului care va fi returnat\n",
        " * shuffle - determina daca exemplele vor fi extrase aleatoriu sau nu\n",
        " * num_workers - numarul de procese paralele care vor incarca datele\n",
        " * collate_fn - o functie care face preprocesari pe N elemente returnate de obiectul de tip Dataset si le concateneaza intr-un batch. N = batch_size\n",
        "\n",
        "Clasa DataLoader implementeaza si functia *\\_\\_len\\_\\_()* pentru a returna numarul de batch-uri din dataset.\n",
        "\n",
        "Obiectul de tip Dataset creat anterior returneaza un tuple-uri care contin o imagine de tip PIL.Image si clasa imaginii de tip int. O retea neurala din Pytorch opereaza pe tipul de date torch.Tensor. Prin urmare obiectul de tip DataLoader trebuie sa returneze obiecte de tip torch.Tensor \n",
        "\n",
        "### Cerinte\n",
        "  1. Iterati prin cele doua obiecte de tip DataLoader si printati doar primul element. (0.5p)\n",
        "  2. Printati shape-ul celor 2 tensori  doar pentru primul element (0.5p)\n",
        "\n",
        "#### Hints\n",
        "  * functia __to_tensor__ din *torchvision.transforms.functional* creaza un obiect de tip torch.Tensor dintr-un obiect de tip PIL.Image\n",
        "  * functia __torch.tensor__ creaza un obiect de tip torch.Tensor dintr-un obiect de tip np.ndarray\n",
        "  * functia __unsqueeze()__ din clasa torch.Tensor creaza o noua dimensiune intr-un tensor. Aceasta este echivalentul functiei __expand_dims()__ din numpy. Exemplu: Daca avem un obiect de tip torch.Tensor, *t*, care contine un vector cu 10 elemente (shape [10]), *t.unsqueeze(0)* va returna un boiect cu aceleasi valori dar cu shape-ul [1, 10]\n",
        "  * functia __torch.cat(tensors, dim=0)__ primeste o lista de tensori si ii concateneaza de-a lungul dimensiunii *dim*. Exemplu: functia primeste o lista cu doi vectori cu shape-ul [1, 10] si *dim=0*, rezultatul are shape-ul [2, 10] (batch 2 ?). Daca *dim=1*, rezultatul va fi [1, 20]. Echivalentul numpy este functia __concatenate()__\n",
        "  * functia __size()__ din clasa torch.Tensor returneaza shape-ul tensorului"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C_MvX_MIYMv"
      },
      "source": [
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "def preproc_fn(examples):\n",
        "  \"\"\"\n",
        "    Functia primeste un batch de exemple pe care trebuie sa le transforme in tensori\n",
        "      si sa le puna intr-un batch de tip torch.Tensor.\n",
        "  \"\"\"\n",
        "  processed_images = []\n",
        "  processed_labels = []\n",
        "\n",
        "  print(processed_images)\n",
        "\n",
        "  for example in examples: # example este un tuplu returnat de obiectul de tip Dataset\n",
        "    pil_image = example[0]\n",
        "    #pil_image_array = np.asarray(pil_image)\n",
        "\n",
        "    tensor_image = to_tensor(pil_image)  # Transforma in obiect de tip torch.Tensor imaginea din example -> 32 x 32 x 3\n",
        "    tensor_image = tensor_image.unsqueeze(0) # Adauga inca o dimensiune la inceputul imaginii -> 1 x 32 x 32 x 3\n",
        "    processed_images.append(tensor_image)\n",
        "\n",
        "    label = np.array([example[1]])# Creaza un obiect de tip np.ndarray din labelul exemplului\n",
        "    tensor_label = torch.Tensor(label)# Creaza un obiect de tip torch.Tensor din label\n",
        "    tensor_label = tensor_label.unsqueeze(0) # Adauga inca o dimensiune la incepului labelului\n",
        "    processed_labels.append(tensor_label)\n",
        "\n",
        "  torch_images = torch.cat(processed_images,  dim=0) \n",
        "  torch_labels = torch.cat(processed_labels, dim=0) \n",
        "\n",
        "  return torch_images, torch_labels\n",
        "\n",
        "loader1 = data.DataLoader(cifar_train, batch_size=1000, shuffle=True, collate_fn=preproc_fn)\n",
        "\n",
        "print(\"Datasetul contine {} de batch-uri\".format(len(loader1)))\n",
        "\n",
        "loader2 = data.DataLoader(cifar_train, batch_size=1000, shuffle=True, num_workers=2, collate_fn=preproc_fn)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for index, batch in enumerate(loader1):\n",
        "\n",
        "#TODO: Cerintele 1 si 2 - Iterati prin loader1 si printati doar primul element si shape-ul celor 2 tensori din exemplu\n",
        "\n",
        "end = time.time()\n",
        "print(\"Iterarea prin dataset cu worker-ul pe procesul curent dureaza {} secunde\".format(end - start))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for index, batch in enumerate(loader2):\n",
        "#TODO: Cerintele 1 si 2 - Iterati prin loader2 si printati doar primul element si shape-ul celor 2 tensori din exemplu\n",
        "\n",
        "end = time.time()\n",
        "print(\"Iterarea prin dataset cu 2 worker-i pe procese diferinte dureaza {} secunde\".format(end - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaJ0EJXOIAjR"
      },
      "source": [
        "## Definirea unei retele cu un singur layer\n",
        "\n",
        "Modulul *torch.nn* contine clase si functii utilitare pentru crearea retelelor neurale. Pentru a crea o retea neurala se va defini o clasa, *SingleLayerNet* ce mosteneste din clasa *torch.nn.Module*. Clasele din Pytorch ce definesc straturi, functii de activare, si functii cost mostenesc din clasa *torch.nn.Module*. Aceste clase implementeaza metoda **forward()** care este folosita pentru a defini ce se intampla la un forward pass. Aceasta metoda este apelata in metoda __\\_\\_call\\_\\_()__ a clasei.\n",
        "\n",
        "Clasa *Linear* din *torch.nn*, ce mosteneste din *nn.Module* defineste un strat 'fully-connected'. Contructorul primeste 3 parametrii:\n",
        " * Dimensiunea vectorului de intrare\n",
        " * Dimensiunea vectorului de iesire\n",
        " * Daca sa se foloseasca *bias* sau nu\n",
        "\n",
        "Clasa Simgoid din *torch.nn* , ce mosteneste din *nn.Module*, defineste o functie de activare sigmoid.\n",
        "\n",
        "### Cerinte\n",
        "  1. In constructorul clasei *SingleLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Linear*\n",
        "  2. In constructorul clase *SingleLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Sigmoid*\n",
        "  3. In metoda __forward()__ definiti o variabila care sa contina iesirea stratului linear aplicat pe intrarea 'x'.\n",
        "  4. In metoda __forward()__ definiti o variabila care sa contina iesirea functiei de activare sigmoid aplicata pe iesirea stratului linear si returnati aceasta variabila\n",
        "\n",
        "#### Hint\n",
        " * Imaginea de intrare are dimensiune 32x32x3 (inaltime x latime x canale). Vectorul de intrare in retea va avea dimensiune 3072.\n",
        "\n",
        "#### Atentie\n",
        " * Dimensiunea de iesire a stratului trebuie sa fie de aceeasi marime cu numarul de clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrSKc8_Hplb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c8f579-c151-44ae-e761-dee95a678285"
      },
      "source": [
        "### Exemplu utilizare metoda __call__()\n",
        "class A(object):\n",
        "  def __init__(self):\n",
        "    self.a ='A'\n",
        "\n",
        "  def __call__(self, mesaj):\n",
        "    print(self.a, mesaj)\n",
        "\n",
        "obj = A()\n",
        "obj(\"OK!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vL3FhX5Xi2j"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SingleLayerNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SingleLayerNet, self).__init__()\n",
        "\n",
        "\n",
        "    self.linear1 = nn.Linear(3072, 10, bias=False)\n",
        "    self.activation1 = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.activation1(self.linear1(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB7O0mVkG231"
      },
      "source": [
        "# Definirea unei retele cu doua straturi (layere). (2p)\n",
        "\n",
        "### Cerinte\n",
        "  1. In constructorul clasei *TwoLayerNet* definiti doua atribute care sa contina doua obiecte de tip *nn.Linear*. (0.5p)\n",
        "  2. In constructorul clasei *TwoLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Sigmoid* (se poate folosi acelasi obiect de tip *nn.Sigmoid* pentru activarea ambelor straturi *nn.Linear*) (0.5p)\n",
        "  3. Implementati metoda **forward()** similar cu exercitul anterior (layer1->activation->layer2->activation). (1p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVAxeNFXHURH"
      },
      "source": [
        "class TwoLayerNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TwoLayerNet, self).__init__()\n",
        "    # Cerinta 1 - completati codul aici\n",
        "    #self.linear1 = ...\n",
        "    #self.linear2 = ...\n",
        "\n",
        "    # Cerinta 2 - completati codul aici\n",
        "    #self.activation1 = ...\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    # Cerinta 3 si 4- completati codul aici\n",
        "\n",
        "    #return output\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZNoELFZrRpC"
      },
      "source": [
        "## Preprocesare pentru retele cu straturi 'fully-connected' (2.5 p)\n",
        "\n",
        "Avand o retea cu straturi 'fully-connected' este necesar ca imaginea sa fie redimensionata intr-un vector. Clasa *torch.Tensor* defineste metoda __view()__ care returneaza un tensor redimensionat.\n",
        "\n",
        "### Cerinte\n",
        "  1. Completati functia de mai jos pentru a redimensiona imaginea intr-un vector. (1p)\n",
        "  2. Extrageti un batch din DataLoader-ul de antrenare si printati dimensiunile imaginii. (1.5p)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDsD6hSs8zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab8c30a-be8a-4728-816d-ac29327fd766"
      },
      "source": [
        "# Varianta numpy\n",
        "img1 = np.random.rand(32, 32, 3)\n",
        "img2 = np.random.rand(32, 32, 3)\n",
        "\n",
        "print(img1.shape, img2.shape)\n",
        "\n",
        "reshaped1 = img1.reshape(-1)\n",
        "reshaped2 = img2.reshape(32*32*3)\n",
        "\n",
        "print(reshaped1.shape, reshaped2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3) (32, 32, 3)\n",
            "(3072,) (3072,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctqQtdWgW-7V"
      },
      "source": [
        "from torchvision.transforms.functional import normalize\n",
        "\n",
        "def preproc_liniarized_fn(examples):\n",
        "  processed_images = []\n",
        "  processed_labels = []\n",
        "\n",
        "  for example in examples:\n",
        "    tensor_image = to_tensor(example[0])\n",
        "    # In linia de mai jos imaginea este normalizata astfel incat sa aiba toate valorile in \n",
        "    # [-1, 1] in loc de [0, 255]\n",
        "    normalized_tensor_image = normalize(tensor_image, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    #TODO: Cerinta 1 - completati codul aici \n",
        "    #vector_image = ...\n",
        "    vector_image = vector_image.unsqueeze(0)\n",
        "    processed_images.append(vector_image)\n",
        "    \n",
        "    label = np.array(example[1])\n",
        "    tensor_label = torch.tensor(label)\n",
        "    tensor_label = tensor_label.unsqueeze(0)\n",
        "    processed_labels.append(tensor_label)\n",
        "\n",
        "  torch_images = torch.cat(processed_images, dim=0)\n",
        "  torch_labels = torch.cat(processed_labels, dim=0)\n",
        "\n",
        "  return torch_images, torch_labels\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = data.DataLoader(cifar_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=preproc_liniarized_fn)\n",
        "test_loader = data.DataLoader(cifar_test, batch_size=1, shuffle=False, collate_fn=preproc_liniarized_fn)\n",
        "\n",
        "#TODO: Cerinta 2 - completati codul aici\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzQgrTWBvvPN"
      },
      "source": [
        "## Definirea antrenarii\n",
        "\n",
        "Pentru definirea antrenarii avem urmatorii pasi:\n",
        " * Definirea numarului de epoci (de cate ori parcurgem intregul dataset)\n",
        " * Definirea obiectului de tip *SingleLayerNet*\n",
        " * Definirea optimizatorului. Vom folosi Stochastic Gradient Descent (SGD) pentru optimizarea retelei, prin urmare definim un obiect de tip *optim.SGD*. Constructorul acestei clase primeste parametrii pe care trebuie sa-i optimizeze (single_layer_net.parameters()) si rata de invatare (lr=1e-2)\n",
        " * Definim functia cost de tip *nn.CrossEntropyLoss()*\n",
        " * Definim functa care parcurge datasetul si antreneaza reteaua."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqAWKYNqZeuS"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Definim numarul de epoci\n",
        "epochs = 10\n",
        "\n",
        "# Definim reteaua\n",
        "single_layer_net = SingleLayerNet()\n",
        "\n",
        "# Definim optimizatorul\n",
        "optimizer = optim.SGD(single_layer_net.parameters(), lr=1e-2)\n",
        "# Dupa definirea optimizatorului si dupa fiecare iteratie trebuie apelata functia zero_grad().\n",
        "# Aceasta face toti gradientii zero.\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Definim functia cost\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_fn(epochs: int, train_loader: data.DataLoader, test_loader: data.DataLoader, \n",
        "             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer):\n",
        "  # Iteram prin numarul de epoci\n",
        "  for e in range(epochs):\n",
        "    # Iteram prin fiecare exemplu din dataset\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "      # Aplicam reteaua neurala pe imaginile de intrare\n",
        "      out = net(images)\n",
        "      # Aplicam functia cost pe iesirea retelei neurale si pe adnotarile imaginilor \n",
        "      loss = loss_fn(out, labels)\n",
        "      # Aplicam algoritmul de back-propagation\n",
        "      loss.backward()\n",
        "      # Facem pasul de optimizare, pentru a aplica gradientii pe parametrii retelei\n",
        "      optimizer.step()\n",
        "      # Apelam functia zero_grad() pentru a uita gradientii de la iteratie curenta\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "    print(\"Loss-ul la finalul epocii {} are valoarea {}\".format(e, loss.item()))\n",
        "\n",
        "    # Calculul acuratetii\n",
        "    count = len(test_loader)\n",
        "    correct = 0\n",
        "\n",
        "    for test_image, test_label in test_loader:\n",
        "      out_class = torch.argmax(net(test_image))\n",
        "      if out_class == test_label:\n",
        "        correct += 1\n",
        "\n",
        "    print(\"Acuratetea la finalul epocii {} este {:.2f}%\".format(e, (correct / count) * batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE-xJ2b52Dw9"
      },
      "source": [
        "## Antrenam propria retea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwo4dQ9b_AxM"
      },
      "source": [
        "train_fn(epochs, train_loader, test_loader, single_layer_net, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAGFBjaD2Ji6"
      },
      "source": [
        "## Definirea unei retele cu 2 straturi si antrenarea ei (2.5 p)\n",
        "\n",
        "### Cerinte\n",
        " 1. Instantiati un obiect de tip *TwoLayerNet*. (0.5p)\n",
        " 2. Definiti un optimizator pentru antrenarea acestei retele (1p)\n",
        " 3. Folositi functia deifnita mai sus pentru a antrena aceasta retea (*train_fn*). (1p)\n",
        "\n",
        "#### Atentie\n",
        " * Dimensiunea de iesire a primului strat trebuie sa se potriveasca cu dimensiunea de intrare a celui de-al doilea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZbXpt9KI6yT"
      },
      "source": [
        "#TODO: Cerinta 1 - completati codul aici\n",
        "\n",
        "# Instantierea retelei\n",
        "# two_layer_net = ...\n",
        "\n",
        "#TODO: Cerinta 2 - completati codul aici\n",
        "#optimizer2 = ...\n",
        "\n",
        "# Dupa definirea optimizatorului si dupa fiecare iteratie trebuie apelata functia zero_grad().\n",
        "# Aceasta face toti gradientii zero.\n",
        "optimizer2.zero_grad()\n",
        "\n",
        "# Definim functia de cost\n",
        "loss_fn2 = nn.CrossEntropyLoss()\n",
        "\n",
        "#TODO: Cerinta 3 - Antrenati reteaua\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICxUifGp3PY1"
      },
      "source": [
        "## Crearea dinamica a unei retele\n",
        "\n",
        "In  *torch.nn* exista clasa *Sequential* care primeste o lista de straturi si functii de activare in ordinea in care trebuie aplicate, e.g. [linear, sigmoid, linear, sigmoid]. Rezultatul este inlantuirea acestor straturi si functii de activare.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vekIJr0pjfcv"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self, layer_sizes: list, activation: type):\n",
        "    \"\"\"\n",
        "      Constructor.\n",
        "\n",
        "      :param layer_sizes - Parametru de tip lista care contine dimensiunile fiecarui strat din retea\n",
        "      :param activation - Parametru de tip type. Poate fi nn.Sigmoid, nn.Tanh, nn.ReLU. Adica clasa pentru a instantia mai tarziu\n",
        "    \"\"\"\n",
        "    super(Net, self).__init__()\n",
        "    \n",
        "    layers = []\n",
        "   \n",
        "    for i in range(0, len(layer_sizes)):\n",
        "      inl, out = layer_sizes[i]\n",
        "      layers.append(nn.Linear(inl, out))\n",
        "      layers.append(activation)\n",
        "\n",
        "    self.net = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RTE0XJg5ppw"
      },
      "source": [
        "## [BONUS] Antrenarea retelei cu N straturi (2p)\n",
        "\n",
        "Mai jos functia de antrenare a fost modificata pentru a afisa modificarile parametrilor retelei in timpul antrenarii. Aceasta se obtine prin implementarea functiei __plot_weights()__.\n",
        "\n",
        "### Cerinte \n",
        "\n",
        "  1. Creati un obiect de tipul *Net* (0.25)\n",
        "  2. Creati un optimizator pentru reteaua de tipul *Net* (0.25)\n",
        "  3. Antrenati reteaua folosind functia __plotting_train_fn()__ (0.25)\n",
        "  4. Experimentati cu retele straturi si functii de activare diferite (0.75)\n",
        "  5. Modificati celula de mai jos si scrieti o functie numita __plot_loss()__ pentru a afisa un grafic care arata evolutia rezultatului functiei cost in timp. Faceti acelasi lucru si pentru acuratete. (0.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ8vr8fjoEs-"
      },
      "source": [
        "def plot_weights(net: nn.Module):\n",
        "  named_params = net.named_parameters()\n",
        "  np_params = []\n",
        "  np_param_names = []\n",
        "  for name, param in named_params:\n",
        "    np_params.append(param.clone().detach().view(-1).numpy())\n",
        "    np_param_names.append(name)\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 2.5))\n",
        "\n",
        "  count = len(np_param_names)\n",
        "  for i in range(count):\n",
        "    plt.subplot(1, count, i+1)\n",
        "    plt.hist(np_params[i], bins=25)\n",
        "    plt.title(np_param_names[i])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plotting_train_fn(epochs: int, train_loader: data.DataLoader, test_loader: data.DataLoader, \n",
        "             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer):\n",
        "  for e in range(epochs):\n",
        "    for images, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      out = net(images)\n",
        "      loss = loss_fn(out, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    \n",
        "    print(\"Loss-ul la finalul epocii {} are valoarea {}\".format(e, loss.item()))\n",
        "\n",
        "    count = len(test_loader)\n",
        "    correct = 0\n",
        "\n",
        "    for test_image, test_label in test_loader:\n",
        "      out_class = torch.argmax(net(test_image))\n",
        "      if out_class == test_label:\n",
        "        correct += 1\n",
        "    \n",
        "    print(\"Acuratetea la finalul epocii {} este {:.2f}%\".format(e, (correct / count) * 100))\n",
        "    plot_weights(net)\n",
        "\n",
        "# Cerinta 1 - completati codul aici\n",
        "\n",
        "# Cerinta 2 - completati codul aici\n",
        "\n",
        "# Cerinta 3 - completati codul aici\n",
        "\n",
        "# Cerinta 5 - completati codul aici\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}